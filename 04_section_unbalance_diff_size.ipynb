{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section:** Unbalance different size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbcabral/miniconda2/envs/howfar/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.ranking module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools as it\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from libs.container import Container\n",
    "from libs.nearest import nearest\n",
    "from libs.experiment import WithAnotherExperiment, roc, metrics\n",
    "from libs.precstar import  prec_star\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = pathlib.Path(os.path.abspath(os.path.dirname(\"\")))\n",
    "\n",
    "DATA_PATH = PATH / \"_data\"\n",
    "\n",
    "COLUMNS_NO_FEATURES = ['id', 'tile', 'cnt', 'ra_k', 'dec_k', 'vs_type', 'vs_catalog', 'cls'] \n",
    "\n",
    "tiles = [\"b234\", \"b360\", \"b278\", \"b261\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 7s, sys: 37.6 s, total: 3min 45s\n",
      "Wall time: 2min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'b234': 537221, 'b261': 732348, 'b278': 853929, 'b360': 1022242}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "DATA_NAMES = {\n",
    "    \"s20k_scaled.pkl.bz2\": 'Large',\n",
    "    \"s5k_scaled.pkl.bz2\": 'Mid',\n",
    "    \"s2_5k_scaled.pkl.bz2\": 'Small',\n",
    "    \"sO2O_scaled.pkl.bz2\": 'One-to-One',\n",
    "    \"full_scaled.pkl.bz2\": \"Full\"\n",
    "}\n",
    "\n",
    "train_datas = {}\n",
    "for path in DATA_PATH.glob(\"*_scaled.pkl.bz2\"):\n",
    "    sample = pd.read_pickle(path)\n",
    "    \n",
    "    # the features\n",
    "    X_columns = [c for c in sample.columns if c not in COLUMNS_NO_FEATURES]\n",
    "    y_column = \"cls\"\n",
    "\n",
    "    sample[X_columns] =  sample[X_columns].astype(np.float32)\n",
    "    \n",
    "    data_name = DATA_NAMES[path.name]\n",
    "    train_datas[data_name] = Container({k: v for k, v in sample.groupby(\"tile\") if k in tiles})\n",
    "\n",
    "    del sample\n",
    "\n",
    "REAL_SIZES = {k: len(v) for k, v in train_datas[\"Full\"].items()}\n",
    "REAL_SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/jbcabral/how_far_can_we_go/_data/s20k.pkl.bz2...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/s5k.pkl.bz2...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/s2_5k.pkl.bz2...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/sO2O.pkl.bz2...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/full.pkl.bz2...\n",
      "CPU times: user 1min 38s, sys: 1.66 s, total: 1min 39s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DATA_UNSC_NAMES = {\n",
    "    \"s20k.pkl.bz2\": 'Large',\n",
    "    \"s5k.pkl.bz2\": 'Mid',\n",
    "    \"s2_5k.pkl.bz2\": 'Small',\n",
    "    \"sO2O.pkl.bz2\": 'One-to-One',\n",
    "    \"full.pkl.bz2\": \"Full\"\n",
    "}\n",
    "\n",
    "unscaled_datas = {}\n",
    "for path in DATA_PATH.glob(\"*.pkl.bz2\"):\n",
    "    if path.name.endswith(\"_scaled.pkl.bz2\"):\n",
    "        continue\n",
    "    print(f\"Reading {path}...\")\n",
    "          \n",
    "    sample = pd.read_pickle(path)\n",
    "    \n",
    "    # the features\n",
    "    X_columns = [c for c in sample.columns if c not in COLUMNS_NO_FEATURES]\n",
    "    y_column = \"cls\"\n",
    "\n",
    "    sample[X_columns] =  sample[X_columns].astype(np.float32)\n",
    "    \n",
    "    data_name = DATA_UNSC_NAMES[path.name]\n",
    "    unscaled_datas[data_name] = sample[sample.tile.isin(tiles)]\n",
    "\n",
    "    del sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_s20k.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_s5k.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_s2_5k.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_sO2O.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_full.pkl...\n",
      "CPU times: user 4.07 ms, sys: 2.18 ms, total: 6.26 ms\n",
      "Wall time: 5.52 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SCALER_NAMES = {\n",
    "    \"scaler_s20k.pkl\": 'Large',\n",
    "    \"scaler_s5k.pkl\": 'Mid',\n",
    "    \"scaler_s2_5k.pkl\": 'Small',\n",
    "    \"scaler_sO2O.pkl\": 'One-to-One',\n",
    "    \"scaler_full.pkl\": \"Full\"\n",
    "}\n",
    "\n",
    "scalers = {}\n",
    "for path in DATA_PATH.glob(\"*.pkl\"):\n",
    "    print(f\"Reading {path}...\")\n",
    "    with open(path, \"rb\") as fp:\n",
    "        name = SCALER_NAMES[path.name]\n",
    "        scalers[name] = pickle.load(fp)\n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the classifiers with the selected parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_PARAMS = {\n",
    "    'criterion': 'entropy',\n",
    "    'max_features': 'log2',\n",
    "    'min_samples_split': 5,\n",
    "    'n_estimators': 500}\n",
    "\n",
    "\n",
    "def make_clf(k, df, X_columns):    \n",
    "    X_train = df[X_columns].values\n",
    "    y_train = df.cls.values\n",
    "\n",
    "    clf = RandomForestClassifier(**RF_PARAMS)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return k, clf\n",
    "\n",
    "\n",
    "def get_clfs(data, X_columns):\n",
    "    print(\"Creating classifiers with {} features...\".format(len(X_columns)))\n",
    "    with joblib.Parallel(n_jobs=-1) as jobs:\n",
    "        clfs = jobs(\n",
    "            joblib.delayed(make_clf)(k, d, X_columns)\n",
    "            for k, d in sorted(tqdm.tqdm(data.items())))\n",
    "    return Container(clfs)\n",
    "\n",
    "\n",
    "def get_combs(train_data, test_data, X_columns):\n",
    "    combs = []\n",
    "    clfs = get_clfs(train_data, X_columns)\n",
    "    for train_name, clf in clfs.items():\n",
    "        for test_name in test_data.keys():\n",
    "            if train_name != test_name:\n",
    "                test_sample = test_data[test_name]  # HERE\n",
    "                comb = Container({\n",
    "                    \"idx\": len(combs), \n",
    "                    \"train_name\": train_name, \"clf\": clf,  \n",
    "                    \"test_name\": test_name, \"test_sample\": test_sample,\n",
    "                    \"X_columns\": X_columns, \"y_column\": y_column})\n",
    "                combs.append(comb)\n",
    "    return combs\n",
    "\n",
    "\n",
    "def execute_clf(idx, train_name, clf, test_name, test_sample, X_columns, y_column):\n",
    "    \n",
    "    X_test = test_sample[X_columns].values\n",
    "    y_test = test_sample[y_column].values\n",
    "    \n",
    "    predictions = clf.predict(X_test)\n",
    "    probabilities = clf.predict_proba(X_test)\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(\n",
    "        y_test, 1.-probabilities[:,0], pos_label=1)\n",
    "\n",
    "    prec_rec_curve = metrics.precision_recall_curve(\n",
    "        y_test, 1.- probabilities[:,0], pos_label=1)\n",
    "\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    result = Container({\n",
    "        \"idx\": idx,\n",
    "        \"train_name\": train_name,\n",
    "        \"test_name\": test_name,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'thresh': thresholds,\n",
    "        'roc_auc': roc_auc,\n",
    "        'prec_rec_curve': prec_rec_curve,\n",
    "        'real_cls': y_test,\n",
    "        'predictions': predictions,\n",
    "        'probabilities': probabilities,\n",
    "        'confusion_matrix': metrics.confusion_matrix(y_test, predictions)})    \n",
    "    return result\n",
    "\n",
    "def scale(df, scaler, features):\n",
    "    print(\"Scaling\")\n",
    "    df = df.copy()    \n",
    "    df[features] = scaler.transform(df[features].values)\n",
    "    return Container({k: v for k, v in df.groupby(\"tile\")})\n",
    "\n",
    "\n",
    "def train_and_run(train_data, test_data, scaler):    \n",
    "    test_data = scale(test_data, scaler, X_columns)\n",
    "    \n",
    "    combs = get_combs(train_data, test_data, X_columns)\n",
    "    print(\"Combinaciones: {}\".format(len(combs)))\n",
    "      \n",
    "    print(\"Launching classifiers for {} features...\".format(len(X_columns)))\n",
    "    with joblib.Parallel(n_jobs=-1) as jobs:\n",
    "        results = jobs(\n",
    "            joblib.delayed(execute_clf)(**comb) for comb in tqdm.tqdm(combs))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 9430.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 64 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 15101.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 64 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 50s, sys: 28.9 s, total: 3min 18s\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"One-to-One\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "o2o_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 5704.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 64 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 15827.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 64 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 51s, sys: 29.9 s, total: 3min 21s\n",
      "Wall time: 3min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"Small\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "small_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 10824.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 64 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 13896.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 64 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 52s, sys: 30.3 s, total: 3min 22s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"Mid\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "mid_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 15448.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 64 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 25203.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 64 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 52s, sys: 29.4 s, total: 3min 21s\n",
      "Wall time: 5min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"Large\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "large_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 7348.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 64 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 14004.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 64 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 5s, sys: 30.1 s, total: 3min 35s\n",
      "Wall time: 1h 26min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"Full\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "full_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_cache/unbalanced_diff.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = {\n",
    "    \"One-to-One\": o2o_results,\n",
    "    \"Small\": small_results,\n",
    "    \"Mid\": mid_results,\n",
    "    \"Large\": large_results,\n",
    "    \"Full\": full_results\n",
    "}\n",
    "\n",
    "joblib.dump(all_results, \"_cache/unbalanced_diff.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
