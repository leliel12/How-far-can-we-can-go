{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section:** Unbalance different size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbcabral/miniconda2/envs/howfar/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.ranking module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools as it\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from libs.container import Container\n",
    "from libs.nearest import nearest\n",
    "from libs.experiment import WithAnotherExperiment, roc, metrics\n",
    "from libs.precstar import  prec_star\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = pathlib.Path(os.path.abspath(os.path.dirname(\"\")))\n",
    "\n",
    "DATA_PATH = PATH / \"_data\"\n",
    "\n",
    "COLUMNS_NO_FEATURES = ['id', 'tile', 'cnt', 'ra_k', 'dec_k', 'vs_type', 'vs_catalog', 'cls'] \n",
    "\n",
    "tiles = [\"b234\", \"b360\", \"b278\", \"b261\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 1s, sys: 22.8 s, total: 2min 24s\n",
      "Wall time: 1min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'b234': 293013, 'b261': 555693, 'b278': 742153, 'b360': 939110}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "DATA_NAMES = {\n",
    "    \"s20k_scaled.pkl.bz2\": 'Large',\n",
    "    \"s5k_scaled.pkl.bz2\": 'Mid',\n",
    "    \"s2_5k_scaled.pkl.bz2\": 'Small',\n",
    "    \"sO2O_scaled.pkl.bz2\": 'One-to-One',\n",
    "    \"full_scaled.pkl.bz2\": \"Full\"\n",
    "}\n",
    "\n",
    "paths = it.chain(DATA_PATH.glob(\"data_2/*_scaled.pkl.bz2\"), DATA_PATH.glob(\"full_scaled.pkl.bz2\"))\n",
    "train_datas = {}\n",
    "for path in paths:\n",
    "    sample = pd.read_pickle(path)\n",
    "    \n",
    "    # the features\n",
    "    X_columns = [c for c in sample.columns if c not in COLUMNS_NO_FEATURES]\n",
    "    y_column = \"cls\"\n",
    "\n",
    "    sample[X_columns] =  sample[X_columns].astype(np.float32)\n",
    "    \n",
    "    \n",
    "    data_name = DATA_NAMES[path.name]\n",
    "    train_datas[data_name] = Container({k: v for k, v in sample.groupby(\"tile\") if k in tiles})\n",
    "\n",
    "    del sample\n",
    "\n",
    "REAL_SIZES = {k: len(v) for k, v in train_datas[\"Full\"].items()}\n",
    "REAL_SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/jbcabral/how_far_can_we_go/_data/data_2/s5k.pkl.bz2...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/data_2/s2_5k.pkl.bz2...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/data_2/sO2O.pkl.bz2...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/full.pkl.bz2...\n",
      "CPU times: user 1min 9s, sys: 1.27 s, total: 1min 11s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DATA_UNSC_NAMES = {\n",
    "    \"s20k.pkl.bz2\": 'Large',\n",
    "    \"s5k.pkl.bz2\": 'Mid',\n",
    "    \"s2_5k.pkl.bz2\": 'Small',\n",
    "    \"sO2O.pkl.bz2\": 'One-to-One',\n",
    "    \"full.pkl.bz2\": \"Full\"\n",
    "}\n",
    "\n",
    "paths = it.chain(DATA_PATH.glob(\"data_2/*.pkl.bz2\"), DATA_PATH.glob(\"full.pkl.bz2\"))\n",
    "unscaled_datas = {}\n",
    "for path in paths:\n",
    "    if path.name.endswith(\"_scaled.pkl.bz2\"):\n",
    "        continue\n",
    "    print(f\"Reading {path}...\")\n",
    "          \n",
    "    sample = pd.read_pickle(path)\n",
    "    \n",
    "    # the features\n",
    "#     X_columns_unsc = [c for c in sample.columns if c not in COLUMNS_NO_FEATURES]\n",
    "    y_column = \"cls\"\n",
    "\n",
    "    sample[X_columns] =  sample[X_columns].astype(np.float32)\n",
    "    \n",
    "    data_name = DATA_UNSC_NAMES[path.name]\n",
    "    unscaled_datas[data_name] = sample[sample.tile.isin(tiles)]\n",
    "\n",
    "    del sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_full.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_s20k.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_s5k.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_s2_5k.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_sO2O.pkl...\n",
      "CPU times: user 4.33 ms, sys: 1.1 ms, total: 5.43 ms\n",
      "Wall time: 4.96 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SCALER_NAMES = {\n",
    "    \"scaler_s20k.pkl\": 'Large',\n",
    "    \"scaler_s5k.pkl\": 'Mid',\n",
    "    \"scaler_s2_5k.pkl\": 'Small',\n",
    "    \"scaler_sO2O.pkl\": 'One-to-One',\n",
    "    \"scaler_full.pkl\": \"Full\"\n",
    "}\n",
    "\n",
    "scalers = {}\n",
    "paths = it.chain(DATA_PATH.glob(\"data_2/*.pkl\"), DATA_PATH.glob(\"scaler_full.pkl\"))\n",
    "for path in DATA_PATH.glob(\"*.pkl\"):\n",
    "    print(f\"Reading {path}...\")\n",
    "    with open(path, \"rb\") as fp:\n",
    "        name = SCALER_NAMES[path.name]\n",
    "        scalers[name] = pickle.load(fp)\n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the classifiers with the selected parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_split': 10,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_PARAMS = joblib.load(\"_cache/best_params.pkl.bz2\")[\"rf\"]\n",
    "if \"n_jobs\" in RF_PARAMS:\n",
    "    del RF_PARAMS[\"n_jobs\"]\n",
    "RF_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clf(k, df, X_columns):    \n",
    "    X_train = df[X_columns].values\n",
    "    y_train = df.cls.values\n",
    "\n",
    "    clf = RandomForestClassifier(**RF_PARAMS)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return k, clf\n",
    "\n",
    "\n",
    "def get_clfs(data, X_columns):\n",
    "    print(\"Creating classifiers with {} features...\".format(len(X_columns)))\n",
    "    with joblib.Parallel(n_jobs=-1) as jobs:\n",
    "        clfs = jobs(\n",
    "            joblib.delayed(make_clf)(k, d, X_columns)\n",
    "            for k, d in sorted(tqdm.tqdm(data.items())))\n",
    "    return Container(clfs)\n",
    "\n",
    "\n",
    "def get_combs(train_data, test_data, X_columns):\n",
    "    combs = []\n",
    "    clfs = get_clfs(train_data, X_columns)\n",
    "    for train_name, clf in clfs.items():\n",
    "        for test_name in test_data.keys():\n",
    "            if train_name != test_name:\n",
    "                test_sample = test_data[test_name]  # HERE\n",
    "                comb = Container({\n",
    "                    \"idx\": len(combs), \n",
    "                    \"train_name\": train_name, \"clf\": clf,  \n",
    "                    \"test_name\": test_name, \"test_sample\": test_sample,\n",
    "                    \"X_columns\": X_columns, \"y_column\": y_column})\n",
    "                combs.append(comb)\n",
    "    return combs\n",
    "\n",
    "\n",
    "def execute_clf(idx, train_name, clf, test_name, test_sample, X_columns, y_column):\n",
    "    \n",
    "    X_test = test_sample[X_columns].values\n",
    "    y_test = test_sample[y_column].values\n",
    "    \n",
    "    predictions = clf.predict(X_test)\n",
    "    probabilities = clf.predict_proba(X_test)\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(\n",
    "        y_test, 1.-probabilities[:,0], pos_label=1)\n",
    "\n",
    "    prec_rec_curve = metrics.precision_recall_curve(\n",
    "        y_test, 1.- probabilities[:,0], pos_label=1)\n",
    "\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    result = Container({\n",
    "        \"idx\": idx,\n",
    "        \"train_name\": train_name,\n",
    "        \"test_name\": test_name,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'thresh': thresholds,\n",
    "        'roc_auc': roc_auc,\n",
    "        'prec_rec_curve': prec_rec_curve,\n",
    "        'real_cls': y_test,\n",
    "        'predictions': predictions,\n",
    "        'probabilities': probabilities,\n",
    "        'confusion_matrix': metrics.confusion_matrix(y_test, predictions)})    \n",
    "    return result\n",
    "\n",
    "def scale(df, scaler, features):\n",
    "    print(\"Scaling\")\n",
    "    df = df.copy()    \n",
    "    df[features] = scaler.transform(df[features].values)\n",
    "    return Container({k: v for k, v in df.groupby(\"tile\")})\n",
    "\n",
    "\n",
    "def train_and_run(train_data, test_data, scaler):    \n",
    "    test_data = scale(test_data, scaler, X_columns)\n",
    "    \n",
    "    combs = get_combs(train_data, test_data, X_columns)\n",
    "    print(\"Combinaciones: {}\".format(len(combs)))\n",
    "      \n",
    "    print(\"Launching classifiers for {} features...\".format(len(X_columns)))\n",
    "    with joblib.Parallel(n_jobs=-1) as jobs:\n",
    "        results = jobs(\n",
    "            joblib.delayed(execute_clf)(**comb) for comb in tqdm.tqdm(combs))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 9300.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 12183.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.7 s, sys: 11.6 s, total: 1min 11s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"One-to-One\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "o2o_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 12464.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 14359.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 11.5 s, total: 1min 12s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"Small\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "small_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 12255.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 13729.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 13 s, total: 1min 14s\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"Mid\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "mid_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 10901.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 13706.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 16.5 s, total: 1min 18s\n",
      "Wall time: 4min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"Large\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "large_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 505 ms, sys: 796 ms, total: 1.3 s\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_results = joblib.load(\"_cache/unbalanced_diff.pkl\")[\"Full\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_cache/unbalanced_diff_2.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = {\n",
    "    \"One-to-One\": o2o_results,\n",
    "    \"Small\": small_results,\n",
    "    \"Mid\": mid_results,\n",
    "    \"Large\": large_results,\n",
    "    \"Full\": full_results\n",
    "}\n",
    "\n",
    "joblib.dump(all_results, \"_cache/unbalanced_diff_2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 1, 20, 16, 12, 41, 925920)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
