{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section:** Unbalance different size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbcabral/miniconda2/envs/howfar/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.ranking module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools as it\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from libs.container import Container\n",
    "from libs.nearest import nearest\n",
    "from libs.experiment import WithAnotherExperiment, roc, metrics\n",
    "from libs.precstar import  prec_star\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = pathlib.Path(os.path.abspath(os.path.dirname(\"\")))\n",
    "\n",
    "DATA_PATH = PATH / \"_data\"\n",
    "\n",
    "COLUMNS_NO_FEATURES = ['id', 'tile', 'cnt', 'ra_k', 'dec_k', 'vs_type', 'vs_catalog', 'cls'] \n",
    "\n",
    "tiles = [\"b234\", \"b360\", \"b278\", \"b261\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = np.array([43,45,42,44,48,60,35,11]) -1\n",
    "# np.array(X_columns)[idx]\n",
    "\n",
    "# X_columns = ['Period_fit', 'Psi_eta', 'PeriodLS', 'Psi_CS', 'Skew',\n",
    "#        'n09_jk_color', 'Mean', 'Freq1_harmonics_amplitude_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 33s, sys: 31.3 s, total: 3min 4s\n",
      "Wall time: 1min 50s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'b234': 293013, 'b261': 555693, 'b278': 742153, 'b360': 939110}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "DATA_NAMES = {\n",
    "    \"s20k_scaled.pkl.bz2\": 'Large',\n",
    "    \"s5k_scaled.pkl.bz2\": 'Mid',\n",
    "    \"s2_5k_scaled.pkl.bz2\": 'Small',\n",
    "    \"sO2O_scaled.pkl.bz2\": 'One-to-One',\n",
    "    \"full_scaled.pkl.bz2\": \"Full\"\n",
    "}\n",
    "\n",
    "train_datas = {}\n",
    "for path in DATA_PATH.glob(\"*_scaled.pkl.bz2\"):\n",
    "    sample = pd.read_pickle(path)\n",
    "    \n",
    "    # the features\n",
    "    X_columns = [c for c in sample.columns if c not in COLUMNS_NO_FEATURES]\n",
    "    y_column = \"cls\"\n",
    "\n",
    "    sample[X_columns] =  sample[X_columns].astype(np.float32)\n",
    "    \n",
    "    \n",
    "    data_name = DATA_NAMES[path.name]\n",
    "    train_datas[data_name] = Container({k: v for k, v in sample.groupby(\"tile\") if k in tiles})\n",
    "\n",
    "    del sample\n",
    "\n",
    "REAL_SIZES = {k: len(v) for k, v in train_datas[\"Full\"].items()}\n",
    "REAL_SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/jbcabral/how_far_can_we_go/_data/s20k.pkl.bz2...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/s5k.pkl.bz2...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/s2_5k.pkl.bz2...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/sO2O.pkl.bz2...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/full.pkl.bz2...\n",
      "CPU times: user 1min 22s, sys: 1.48 s, total: 1min 23s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DATA_UNSC_NAMES = {\n",
    "    \"s20k.pkl.bz2\": 'Large',\n",
    "    \"s5k.pkl.bz2\": 'Mid',\n",
    "    \"s2_5k.pkl.bz2\": 'Small',\n",
    "    \"sO2O.pkl.bz2\": 'One-to-One',\n",
    "    \"full.pkl.bz2\": \"Full\"\n",
    "}\n",
    "\n",
    "unscaled_datas = {}\n",
    "for path in DATA_PATH.glob(\"*.pkl.bz2\"):\n",
    "    if path.name.endswith(\"_scaled.pkl.bz2\"):\n",
    "        continue\n",
    "    print(f\"Reading {path}...\")\n",
    "          \n",
    "    sample = pd.read_pickle(path)\n",
    "    \n",
    "    # the features\n",
    "#     X_columns_unsc = [c for c in sample.columns if c not in COLUMNS_NO_FEATURES]\n",
    "    y_column = \"cls\"\n",
    "\n",
    "    sample[X_columns] =  sample[X_columns].astype(np.float32)\n",
    "    \n",
    "    data_name = DATA_UNSC_NAMES[path.name]\n",
    "    unscaled_datas[data_name] = sample[sample.tile.isin(tiles)]\n",
    "\n",
    "    del sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_s20k.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_s5k.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_s2_5k.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_sO2O.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_full.pkl...\n",
      "CPU times: user 3.86 ms, sys: 1.4 ms, total: 5.26 ms\n",
      "Wall time: 5.23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SCALER_NAMES = {\n",
    "    \"scaler_s20k.pkl\": 'Large',\n",
    "    \"scaler_s5k.pkl\": 'Mid',\n",
    "    \"scaler_s2_5k.pkl\": 'Small',\n",
    "    \"scaler_sO2O.pkl\": 'One-to-One',\n",
    "    \"scaler_full.pkl\": \"Full\"\n",
    "}\n",
    "\n",
    "scalers = {}\n",
    "for path in DATA_PATH.glob(\"*.pkl\"):\n",
    "    print(f\"Reading {path}...\")\n",
    "    with open(path, \"rb\") as fp:\n",
    "        name = SCALER_NAMES[path.name]\n",
    "        scalers[name] = pickle.load(fp)\n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the classifiers with the selected parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_features': 'log2',\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_PARAMS = joblib.load(\"_cache/best_params.pkl.bz2\")[\"rf\"]\n",
    "del RF_PARAMS[\"n_jobs\"]\n",
    "RF_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clf(k, df, X_columns):    \n",
    "    X_train = df[X_columns].values\n",
    "    y_train = df.cls.values\n",
    "\n",
    "    clf = RandomForestClassifier(**RF_PARAMS)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return k, clf\n",
    "\n",
    "\n",
    "def get_clfs(data, X_columns):\n",
    "    print(\"Creating classifiers with {} features...\".format(len(X_columns)))\n",
    "    with joblib.Parallel(n_jobs=-1) as jobs:\n",
    "        clfs = jobs(\n",
    "            joblib.delayed(make_clf)(k, d, X_columns)\n",
    "            for k, d in sorted(tqdm.tqdm(data.items())))\n",
    "    return Container(clfs)\n",
    "\n",
    "\n",
    "def get_combs(train_data, test_data, X_columns):\n",
    "    combs = []\n",
    "    clfs = get_clfs(train_data, X_columns)\n",
    "    for train_name, clf in clfs.items():\n",
    "        for test_name in test_data.keys():\n",
    "            if train_name != test_name:\n",
    "                test_sample = test_data[test_name]  # HERE\n",
    "                comb = Container({\n",
    "                    \"idx\": len(combs), \n",
    "                    \"train_name\": train_name, \"clf\": clf,  \n",
    "                    \"test_name\": test_name, \"test_sample\": test_sample,\n",
    "                    \"X_columns\": X_columns, \"y_column\": y_column})\n",
    "                combs.append(comb)\n",
    "    return combs\n",
    "\n",
    "\n",
    "def execute_clf(idx, train_name, clf, test_name, test_sample, X_columns, y_column):\n",
    "    \n",
    "    X_test = test_sample[X_columns].values\n",
    "    y_test = test_sample[y_column].values\n",
    "    \n",
    "    predictions = clf.predict(X_test)\n",
    "    probabilities = clf.predict_proba(X_test)\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(\n",
    "        y_test, 1.-probabilities[:,0], pos_label=1)\n",
    "\n",
    "    prec_rec_curve = metrics.precision_recall_curve(\n",
    "        y_test, 1.- probabilities[:,0], pos_label=1)\n",
    "\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    result = Container({\n",
    "        \"idx\": idx,\n",
    "        \"train_name\": train_name,\n",
    "        \"test_name\": test_name,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'thresh': thresholds,\n",
    "        'roc_auc': roc_auc,\n",
    "        'prec_rec_curve': prec_rec_curve,\n",
    "        'real_cls': y_test,\n",
    "        'predictions': predictions,\n",
    "        'probabilities': probabilities,\n",
    "        'confusion_matrix': metrics.confusion_matrix(y_test, predictions)})    \n",
    "    return result\n",
    "\n",
    "def scale(df, scaler, features):\n",
    "    print(\"Scaling\")\n",
    "    df = df.copy()    \n",
    "    df[features] = scaler.transform(df[features].values)\n",
    "    return Container({k: v for k, v in df.groupby(\"tile\")})\n",
    "\n",
    "\n",
    "def train_and_run(train_data, test_data, scaler):    \n",
    "    test_data = scale(test_data, scaler, X_columns)\n",
    "    \n",
    "    combs = get_combs(train_data, test_data, X_columns)\n",
    "    print(\"Combinaciones: {}\".format(len(combs)))\n",
    "      \n",
    "    print(\"Launching classifiers for {} features...\".format(len(X_columns)))\n",
    "    with joblib.Parallel(n_jobs=-1) as jobs:\n",
    "        results = jobs(\n",
    "            joblib.delayed(execute_clf)(**comb) for comb in tqdm.tqdm(combs))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 10362.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 15782.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.6 s, sys: 11 s, total: 1min 8s\n",
      "Wall time: 3min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"One-to-One\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "o2o_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 17962.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 15927.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.8 s, sys: 10.5 s, total: 1min 10s\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"Small\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "small_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 16561.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 12742.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 10.5 s, total: 1min 12s\n",
      "Wall time: 3min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"Mid\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "mid_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 3955.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 14446.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 12.5 s, total: 1min 20s\n",
      "Wall time: 4min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"Large\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "large_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 8634.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 8730.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 14s, sys: 13.5 s, total: 1min 27s\n",
      "Wall time: 1h 3min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"Full\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "full_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_cache/unbalanced_diff.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = {\n",
    "    \"One-to-One\": o2o_results,\n",
    "    \"Small\": small_results,\n",
    "    \"Mid\": mid_results,\n",
    "    \"Large\": large_results,\n",
    "    \"Full\": full_results\n",
    "}\n",
    "\n",
    "joblib.dump(all_results, \"_cache/unbalanced_diff.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 1, 17, 14, 10, 42, 103548)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.datetime.now()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
