{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section:** Unbalance different size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbcabral/miniconda2/envs/howfar/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.ranking module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools as it\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from libs.container import Container\n",
    "from libs.nearest import nearest\n",
    "from libs.experiment import WithAnotherExperiment, roc, metrics\n",
    "from libs.precstar import  prec_star\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = pathlib.Path(os.path.abspath(os.path.dirname(\"\")))\n",
    "\n",
    "DATA_PATH = PATH / \"_data\"\n",
    "\n",
    "COLUMNS_NO_FEATURES = ['id', 'tile', 'cnt', 'ra_k', 'dec_k', 'vs_type', 'vs_catalog', 'cls'] \n",
    "\n",
    "tiles = [\"b234\", \"b360\", \"b278\", \"b261\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 45s, sys: 24.1 s, total: 3min 9s\n",
      "Wall time: 2min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'b234': 293013, 'b261': 555693, 'b278': 742153, 'b360': 939110}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "DATA_NAMES = {\n",
    "    \"s20k_scaled.pkl.bz2\": 'Large',\n",
    "    \"s5k_scaled.pkl.bz2\": 'Mid',\n",
    "    \"s2_5k_scaled.pkl.bz2\": 'Small',\n",
    "    \"sO2O_scaled.pkl.bz2\": 'One-to-One',\n",
    "    \"s10p_scaled.pkl.bz2\": 'Huge',\n",
    "    \"full_scaled.pkl.bz2\": \"Full\"\n",
    "}\n",
    "\n",
    "paths = it.chain(DATA_PATH.glob(\"data_3/*_scaled.pkl.bz2\"), DATA_PATH.glob(\"full_scaled.pkl.bz2\"))\n",
    "train_datas = {}\n",
    "for path in paths:\n",
    "    sample = pd.read_pickle(path)\n",
    "    \n",
    "    # the features\n",
    "    X_columns = [c for c in sample.columns if c not in COLUMNS_NO_FEATURES]\n",
    "    y_column = \"cls\"\n",
    "\n",
    "    sample[X_columns] =  sample[X_columns].astype(np.float32)\n",
    "    \n",
    "    \n",
    "    data_name = DATA_NAMES[path.name]\n",
    "    train_datas[data_name] = Container({k: v for k, v in sample.groupby(\"tile\") if k in tiles})\n",
    "\n",
    "    del sample\n",
    "\n",
    "REAL_SIZES = {k: len(v) for k, v in train_datas[\"Full\"].items()}\n",
    "REAL_SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/jbcabral/how_far_can_we_go/_data/full.pkl.bz2...\n",
      "CPU times: user 1min 13s, sys: 2.66 s, total: 1min 16s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DATA_UNSC_NAMES = {\n",
    "    \"s20k.pkl.bz2\": 'Large',\n",
    "    \"s5k.pkl.bz2\": 'Mid',\n",
    "    \"s2_5k.pkl.bz2\": 'Small',\n",
    "    \"sO2O.pkl.bz2\": 'One-to-One',\n",
    "    \"s10p.pkl.bz2\": 'Huge',\n",
    "    \"full.pkl.bz2\": \"Full\"\n",
    "}\n",
    "\n",
    "paths = it.chain(DATA_PATH.glob(\"data_3/*.pkl.bz2\"), DATA_PATH.glob(\"full.pkl.bz2\"))\n",
    "paths = DATA_PATH.glob(\"full.pkl.bz2\") # <<<<< WARNING\n",
    "unscaled_datas = {}\n",
    "for path in paths:\n",
    "    if path.name.endswith(\"_scaled.pkl.bz2\"):\n",
    "        continue\n",
    "    print(f\"Reading {path}...\")\n",
    "          \n",
    "    sample = pd.read_pickle(path)\n",
    "    \n",
    "    # the features\n",
    "#     X_columns_unsc = [c for c in sample.columns if c not in COLUMNS_NO_FEATURES]\n",
    "    y_column = \"cls\"\n",
    "\n",
    "    sample[X_columns] =  sample[X_columns].astype(np.float32)\n",
    "    \n",
    "    data_name = DATA_UNSC_NAMES[path.name]\n",
    "    unscaled_datas[data_name] = sample[sample.tile.isin(tiles)]\n",
    "\n",
    "    del sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_full.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_s10p.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_s20k.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_s5k.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_s2_5k.pkl...\n",
      "Reading /home/jbcabral/how_far_can_we_go/_data/scaler_sO2O.pkl...\n",
      "CPU times: user 3.86 ms, sys: 1.99 ms, total: 5.85 ms\n",
      "Wall time: 4.91 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SCALER_NAMES = {\n",
    "    \"scaler_s20k.pkl\": 'Large',\n",
    "    \"scaler_s5k.pkl\": 'Mid',\n",
    "    \"scaler_s2_5k.pkl\": 'Small',\n",
    "    \"scaler_sO2O.pkl\": 'One-to-One',\n",
    "    \"scaler_s10p.pkl\": 'Huge',\n",
    "    \"scaler_full.pkl\": \"Full\"\n",
    "}\n",
    "\n",
    "scalers = {}\n",
    "paths = it.chain(DATA_PATH.glob(\"data_3/*.pkl\"), DATA_PATH.glob(\"scaler_full.pkl\"))\n",
    "for path in DATA_PATH.glob(\"*.pkl\"):\n",
    "    print(f\"Reading {path}...\")\n",
    "    with open(path, \"rb\") as fp:\n",
    "        name = SCALER_NAMES[path.name]\n",
    "        scalers[name] = pickle.load(fp)\n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the classifiers with the selected parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_features': 'log2',\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_PARAMS = joblib.load(\"_cache/best_params.pkl.bz2\")[\"rf\"]\n",
    "if \"n_jobs\" in RF_PARAMS:\n",
    "    del RF_PARAMS[\"n_jobs\"]\n",
    "RF_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clf(k, df, X_columns):    \n",
    "    X_train = df[X_columns].values\n",
    "    y_train = df.cls.values\n",
    "\n",
    "    clf = RandomForestClassifier(**RF_PARAMS)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return k, clf\n",
    "\n",
    "\n",
    "def get_clfs(data, X_columns):\n",
    "    print(\"Creating classifiers with {} features...\".format(len(X_columns)))\n",
    "    with joblib.Parallel(n_jobs=-1) as jobs:\n",
    "        clfs = jobs(\n",
    "            joblib.delayed(make_clf)(k, d, X_columns)\n",
    "            for k, d in sorted(tqdm.tqdm(data.items())))\n",
    "    return Container(clfs)\n",
    "\n",
    "\n",
    "def get_combs(train_data, test_data, X_columns):\n",
    "    combs = []\n",
    "    clfs = get_clfs(train_data, X_columns)\n",
    "    for train_name, clf in clfs.items():\n",
    "        for test_name in test_data.keys():\n",
    "            if train_name != test_name:\n",
    "                test_sample = test_data[test_name]  # HERE\n",
    "                comb = Container({\n",
    "                    \"idx\": len(combs), \n",
    "                    \"train_name\": train_name, \"clf\": clf,  \n",
    "                    \"test_name\": test_name, \"test_sample\": test_sample,\n",
    "                    \"X_columns\": X_columns, \"y_column\": y_column})\n",
    "                combs.append(comb)\n",
    "    return combs\n",
    "\n",
    "\n",
    "def execute_clf(idx, train_name, clf, test_name, test_sample, X_columns, y_column):\n",
    "    \n",
    "    X_test = test_sample[X_columns].values\n",
    "    y_test = test_sample[y_column].values\n",
    "    \n",
    "    predictions = clf.predict(X_test)\n",
    "    probabilities = clf.predict_proba(X_test)\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(\n",
    "        y_test, 1.-probabilities[:,0], pos_label=1)\n",
    "\n",
    "    prec_rec_curve = metrics.precision_recall_curve(\n",
    "        y_test, 1.- probabilities[:,0], pos_label=1)\n",
    "\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    result = Container({\n",
    "        \"idx\": idx,\n",
    "        \"train_name\": train_name,\n",
    "        \"test_name\": test_name,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'thresh': thresholds,\n",
    "        'roc_auc': roc_auc,\n",
    "        'prec_rec_curve': prec_rec_curve,\n",
    "        'real_cls': y_test,\n",
    "        'predictions': predictions,\n",
    "        'probabilities': probabilities,\n",
    "        'confusion_matrix': metrics.confusion_matrix(y_test, predictions)})    \n",
    "    return result\n",
    "\n",
    "def scale(df, scaler, features):\n",
    "    print(\"Scaling\")\n",
    "    df = df.copy()    \n",
    "    df[features] = scaler.transform(df[features].values)\n",
    "    return Container({k: v for k, v in df.groupby(\"tile\")})\n",
    "\n",
    "\n",
    "def train_and_run(train_data, test_data, scaler):    \n",
    "    test_data = scale(test_data, scaler, X_columns)\n",
    "    \n",
    "    combs = get_combs(train_data, test_data, X_columns)\n",
    "    print(\"Combinaciones: {}\".format(len(combs)))\n",
    "      \n",
    "    print(\"Launching classifiers for {} features...\".format(len(X_columns)))\n",
    "    with joblib.Parallel(n_jobs=-1) as jobs:\n",
    "        results = jobs(\n",
    "            joblib.delayed(execute_clf)(**comb) for comb in tqdm.tqdm(combs))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 4984.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 13504.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 29.6 s, total: 2min 11s\n",
      "Wall time: 5min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"One-to-One\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "o2o_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 4865.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 9258.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 42s, sys: 28.2 s, total: 2min 10s\n",
      "Wall time: 5min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"Small\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "small_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 5181.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 7302.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 39s, sys: 21.9 s, total: 2min 1s\n",
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"Mid\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "mid_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 8751.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 7931.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 12.9 s, total: 1min 25s\n",
      "Wall time: 6min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"Large\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "large_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 6795.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classifiers with 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:00<00:00, 10472.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones: 12\n",
      "Launching classifiers for 62 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 14s, sys: 14.7 s, total: 1min 29s\n",
      "Wall time: 11min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_name, test_name = \"Huge\", \"Full\"\n",
    "train_data = train_datas[train_name]\n",
    "scaler = scalers[train_name]\n",
    "test_data = unscaled_datas[test_name]\n",
    "                           \n",
    "huge_results = train_and_run(train_data=train_data, test_data=test_data, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 596 ms, sys: 598 ms, total: 1.19 s\n",
      "Wall time: 3.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_results = joblib.load(\"_cache/unbalanced_diff.pkl\")[\"Full\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_cache/unbalanced_diff_3.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = {\n",
    "    \"One-to-One\": o2o_results,\n",
    "    \"Small\": small_results,\n",
    "    \"Mid\": mid_results,\n",
    "    \"Large\": large_results,\n",
    "    \"Huge\": huge_results,\n",
    "    \"Full\": full_results\n",
    "}\n",
    "\n",
    "joblib.dump(all_results, \"_cache/unbalanced_diff_3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 1, 23, 13, 7, 58, 972665)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
